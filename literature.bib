@misc{citationrequired,
  title  = {Citation required},
  author = {placeholder}
}

@techreport{sampath2020low,
  title       = {Low Complexity Acoustic Scene Classification Using Aalnet-94},
  author      = {Sampathkumar, Arunodhayan and Kowerko, Danny},
  institution = {DCASE2020 Challenge},
  year        = {2020},
  month       = jun,
  url         = {https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Sampathkumar_42.pdf},
  abstract    = {One of the manifold application fields of Deep Neural Networks (DNN) is the classification of audio signals such as indoor, outdoor, transportation, humans and animals sounds. DCASE2020 provided a dataset consisting of 3 classes to perform classification using low complexity solutions. The dataset was trained using AALNet-94 from our previous research work that performed well in publicly available datasets such as ESC-50, Ultrasound 8K and audioset. The results obtained performed well when compared with the baseline.}
}

@article{sampath2019cnn,
  title   = {CNN-based Audio Classification for Environmental Sounds, Ambient Assisted Living and Public Transport Environments using an Extensive Combined Dataset},
  author  = {Sampathkumar, A and Erler, R and Kowerko, D},
  journal = {Chemnitzer Informatik Berichte},
  volume  = {CSR-20-01},
  pages   = {29--66},
  year    = {2019},
  month   = jan,
  url     = {https://www.tu-chemnitz.de/informatik/service/ib/pdf/CSR-20-01.pdf}
}

@inproceedings{sampath2019realtime,
  doi       = {10.1145/3343031.3350600},
  url       = {https://doi.org/10.1145/3343031.3350600},
  year      = {2019},
  month     = oct,
  publisher = {{ACM}},
  author    = {Arunodhayan Sampathkumar and Ren{\'{e}} Erler and Danny Kowerko},
  title     = {A Real-Time Demo for Acoustic Event Classification in Ambient Assisted Living Contexts},
  booktitle = {Proceedings of the 27th {ACM} International Conference on Multimedia},
  pages     = {2205--2207}
}

@book{10.5555/3202425,
  author    = {Nimmagadda, Muralidhar},
  title     = {CRACKING DESIGN INTERVIEWS: System Design},
  year      = {2017},
  isbn      = {1973212838},
  publisher = {Independently published},
  abstract  = {Are you preparing for technical interviews? Do you know the number one cause of people failing to crack interviews is lack of preparation? Though coding is still the major part of technical interviews, companies these days are including atleast one system design question to check the expertise of the candidate in designing large scale systems. For example :- careers page of facebook clearly mentions there will be one round of system design interview. Sample questions will be like "Design Twitter" or "Design an e-commerce website like amazon". So, How do you prepare to tackle such tough questions in interviews? Unfortunately, there are no good resources to learn system design. Part of it comes through practical experience and part of it from understanding various architectures and tradeoffs. Added to that, in most cases there wont be a single solution to the problem. Depending on the conversation and interviewer, interview can go in any direction and may go deep into certain areas. So, it makes preparing for system design interviews very challenging. This book is written primarily to help candidates get ready for the system design interview in short period of time. It provides step-by-step approach ( 10 steps ) to navigate through any system design interview effortlessly. It also provides guidance on how to design each layer of software systems like Storage Layer, Cache Layer, Application Layer, Web Layer, Client Layer etc. It covers topics like High-Availability, Scalability, Consistency that are important properties of any software system. It also provides sample solutions for designing write-heavy systems like dropbox and read-heavy systems like twitter. Check it out. All the best. Happy interviewing.},
  url       = {https://dl.acm.org/doi/book/10.5555/3202425}
}

@article{SHARAN201724,
  title    = {Robust acoustic event classification using deep neural networks},
  journal  = {Information Sciences},
  volume   = {396},
  pages    = {24-32},
  year     = {2017},
  issn     = {0020-0255},
  doi      = {https://doi.org/10.1016/j.ins.2017.02.013},
  url      = {https://www.sciencedirect.com/science/article/pii/S0020025517304553},
  author   = {Roneel V Sharan and Tom J Moir},
  keywords = {Sound event recognition, Time-frequency image feature, Deep neural networks, Support vector machines},
  abstract = {Support vector machines (SVMs) have seen an increased usage in applications of acoustic event classification since its rise to popularity about two decades ago. However, in recent years, deep learning methods, such as deep neural networks (DNNs), have shown to outperform a number of classification methods in various pattern recognition problems. This work starts by comparing the classification performance of DNNs against SVMs with a number of feature representations which fall into two categories: cepstral features and time-frequency image features. Unlike related work, the classification performance of the two classifiers is also compared with feature vector combination and the training and evaluation times of the classifiers and features are also compared. The performance is evaluated on an audio surveillance database containing 10 sound classes, each class having multiple subclasses, with the addition of noise at various signal-to-noise ratios (SNRs). The experimental results shows that DNNs have a better overall classification performance than SVMs with both individual and combined features and the classification accuracy with DNNs is particularly better at low SNRs. The evaluation time of the DNN classifier was also determined to be the fastest but with a slow training time.}
}

@article{6857341,
  author  = {Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title   = {Convolutional Neural Networks for Speech Recognition},
  year    = {2014},
  volume  = {22},
  number  = {10},
  pages   = {1533-1545},
  doi     = {10.1109/TASLP.2014.2339736}
}

@inproceedings{9746093,
  author    = {Paissan, Francesco and Ancilotto, Alberto and Brutti, Alessio and Farella, Elisabetta},
  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Scalable Neural Architectures for End-to-End Environmental Sound Classification},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {641-645},
  doi       = {10.1109/ICASSP43922.2022.9746093}
}

@misc{w3AccessibilityUsability,
  author       = {W3C Web Accessibility Initiative (WAI)},
  title        = {{A}ccessibility, {U}sability, and {I}nclusion --- w3.org},
  howpublished = {\url{https://www.w3.org/WAI/fundamentals/accessibility-usability-inclusion/}},
  year         = {},
  note         = {[Accessed 19-11-2023]}
}

@misc{SeniorfriendlyTechnologies,
  author       = {},
  title        = {{S}enior-friendly technologies | {C}{H}{I} '10 {E}xtended {A}bstracts on {H}uman {F}actors in {C}omputing {S}ystems --- doi.org},
  howpublished = {\url{https://doi.org/10.1145/1753846.1754187}},
  year         = {},
  note         = {[Accessed 19-11-2023]}
}
@misc{AWSTimestream,
  title        = {Amazon Timestream – Time Series Database – Amazon Web Services},
  howpublished = {\url{http://web.archive.org/web/20220525195725/https://aws.amazon.com/timestream/}},
  note         = {Accessed: 2022-05-25}
}

@misc{AzureCosmos,
  title        = {Azure Cosmos DB – NoSQL Database | Microsoft Azure},
  howpublished = {\url{http://web.archive.org/web/20220622095016/https://azure.microsoft.com/en-us/services/cosmos-db/}},
  note         = {Accessed: 2022-06-22}
}

@misc{AlertManager,
  title        = {prometheus/alertmanager: Prometheus Alertmanager},
  howpublished = {\url{https://web.archive.org/web/20220629085738/https://github.com/prometheus/alertmanager}},
  note         = {Accessed: 2022-06-29}
}

@misc{kapacitor,
  title        = {influxdata/kapacitor: Open source framework for processing, monitoring, and alerting on time series data},
  howpublished = {\url{https://web.archive.org/web/20220601203012/https://github.com/influxdata/kapacitor}},
  note         = {Accessed: 2022-06-01}
}


@article{moons2019embedded,
  title     = {Embedded deep learning},
  author    = {Moons, Bert and Bankman, Daniel and Verhelst, Marian},
  journal   = {Embedded Deep Learning},
  year      = {2019},
  publisher = {Springer}
}



@misc{mosquittoexporter,
  author       = {},
  title        = {{G}it{H}ub - sapcc/mosquitto-exporter: {P}rometheus metrics exporter for the {M}osquitto message broker --- github.com},
  howpublished = {\url{https://github.com/sapcc/mosquitto-exporter}},
  year         = {},
  note         = {[Accessed 19-11-2023]}
}
@inproceedings{galvis2010messaging,
  title        = {Messaging Design Pattern and Pattern Implementation},
  author       = {Galvis, Al},
  booktitle    = {17th conference on Pattern Languages of Programs-PLoP},
  year         = {2010},
  organization = {Citeseer}
}
@book{hillar2017mqtt,
  title     = {MQTT Essentials-A lightweight IoT protocol},
  author    = {Hillar, Gaston C},
  year      = {2017},
  publisher = {Packt Publishing Ltd}
}

@misc{mosquittoMosquittoconfPage,
  author       = {},
  title        = {mosquitto.conf man page --- mosquitto.org},
  howpublished = {\url{https://mosquitto.org/man/mosquitto-conf-5.html}},
  year         = {},
  note         = {[Accessed 19-11-2023]}
}


@book{turnbull2018monitoring,
  title     = {Monitoring with Prometheus},
  author    = {Turnbull, James},
  year      = {2018},
  publisher = {Turnbull Press}
}


@misc{timescalePrometheusQuerying,
  author       = {},
  title        = {{H}ow {P}rometheus {Q}uerying {W}orks (and {W}hy {Y}ou {S}hould {C}are) --- timescale.com},
  howpublished = {\url{https://www.timescale.com/blog/how-prometheus-querying-works-and-why-you-should-care/}},
  year         = {},
  note         = {[Accessed 19-11-2023]}
}

@misc{reactReact,
  author       = {},
  title        = {{R}eact --- react.dev},
  howpublished = {\url{https://react.dev/}},
  year         = {},
  note         = {[Accessed 21-11-2023]}
}

@misc{reactrouterHomeV6190,
  author       = {},
  title        = {{H}ome v6.19.0 --- reactrouter.com},
  howpublished = {\url{https://reactrouter.com/}},
  year         = {},
  note         = {[Accessed 21-11-2023]}
}

 @inbook{Mishra_2018,
  title     = {Performance Evaluation of MQTT Broker Servers},
  isbn      = {9783319951713},
  issn      = {1611-3349},
  url       = {http://dx.doi.org/10.1007/978-3-319-95171-3_47},
  doi       = {10.1007/978-3-319-95171-3_47},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  author    = {Mishra, Biswajeeban},
  year      = {2018},
  pages     = {599–609}
}
@phdthesis{rothenhausler2022d3,
  title  = {d3. js and its potential in data visualization},
  author = {Rothenh{\"a}usler, Luis},
  year   = {2022},
  school = {Technische Hochschule Brandenburg}
}









@preamble{ "\newcommand{\noopsort}[1]{} "
	# "\newcommand{\printfirst}[2]{#1} "
	# "\newcommand{\singleletter}[1]{#1} "
	# "\newcommand{\switchargs}[2]{#2#1} " }
	


@inproceedings{KHF17_Acoustic_Event_Classification_Using_Convolutional_Neural_Networks,
  author    = {Kahl, Stefan AND Hussein, Hussein AND Fabian, Etienne AND Schloßhauer, Jan AND Thangaraju, Enniyan AND Kowerko, Danny AND Eibl, Maximilian},
  title     = {Acoustic Event Classification Using Convolutional Neural Networks},
  booktitle = {INFORMATIK 2017},
  year      = {2017},
  editor    = {Eibl, Maximilian AND Gaedke, Martin},
  pages     = { 2177-2188 },
  publisher = {Gesellschaft für Informatik, Bonn},
  address   = {Chemnitz, Germany},
  url       = {https://doi.org/10.18420/in2017_217}
}


@inproceedings{Pic15_Environmental_Sound_Classification_with_Convolutional_Neural_Networks,
  author    = {Karol J. Piczak},
  title     = {Environmental sound classification with convolutional neural networks},
  booktitle = {25th {IEEE} International Workshop on Machine Learning for Signal
               Processing, {MLSP} 2015, Boston, MA, USA, September 17-20, 2015},
  pages     = {1--6},
  year      = {2015},
  url       = {https://doi.org/10.1109/MLSP.2015.7324337}
}


@inproceedings{SAP17_Unsupervised_Filterbank_Learning_Using_Convolutional_Restricted_Boltzmann,
  author    = {Hardik B. Sailor and
               Dharmesh M. Agrawal and
               Hemant A. Patil},
  title     = {Unsupervised Filterbank Learning Using Convolutional Restricted Boltzmann
               Machine for Environmental Sound Classification},
  booktitle = {Interspeech 2017, 18th Annual Conference of the International Speech
               Communication Association, Stockholm, Sweden, August 20-24, 2017},
  pages     = {3107--3111},
  year      = {2017},
  url       = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0831.html}
}


@misc{fedden_comparative_nodate,
  title   = {Comparative {Audio} {Analysis} {With} {Wavenet}, {MFCCs}, {UMAP}, t-{SNE} and {PCA}},
  url     = {https://medium.com/@LeonFedden/comparative-audio-analysis-with-wavenet-mfccs-umap-t-sne-and-pca-cb8237bfce2f},
  urldate = {2018-12-11},
  journal = {Comparative Audio Analysis With Wavenet, MFCCs, UMAP, t-SNE and PCA},
  author  = {Fedden, Leon}
}








